{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae45b54-1241-4610-a608-14b778319623",
   "metadata": {},
   "source": [
    "### Check whether gpu available or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35317f0-b469-4575-9ff2-83a12a2716ab",
   "metadata": {},
   "source": [
    "#### For Nvidia system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc5eaa-64fa-4715-8bd0-c126f0ba56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7b947-083c-4f3c-80eb-b53befa5cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available for Apple Silicon\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU is available for acceleration.')\n",
    "    device = torch.device(\"cuda\")  # Use MPS backend\n",
    "else:\n",
    "    print('GPU is not available. Using CPU.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print('Selected device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3fc35b-a583-4abd-b9af-354dad8fd57a",
   "metadata": {},
   "source": [
    "#### For Apple Silicon system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad9033-ab27-4753-8349-7f29e9d9167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available for Apple Silicon\n",
    "if torch.backends.mps.is_available():\n",
    "    print('Metal is available for acceleration.')\n",
    "    device = torch.device(\"mps\")  # Use MPS backend\n",
    "else:\n",
    "    print('Metal is not available. Using CPU.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print('Selected device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bff166",
   "metadata": {},
   "source": [
    "#### Import necessary libraries and install required packages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c462f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchmetrics transformers sklearn nltk pytorch -qU\n",
    "%pip install transformers\n",
    "%pip install sklearn\n",
    "\n",
    "%pip install scikit-learn\n",
    "%pip install datasets\n",
    "%pip install nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa1b94d",
   "metadata": {},
   "source": [
    "#### Load the dataset from CSV files for training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"/train_set.csv\",\n",
    "              \"valid\": \"/validate_set.csv\",\n",
    "              \"test\": \"/test_set.csv\"}\n",
    "dataset = load_dataset('csv', data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ba1fc",
   "metadata": {},
   "source": [
    "#### Set up Weights and Biases for Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WANDB_PROJECT=roberta_base\n",
    "%env WANDB_LOG_MODEL=\"end\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"FeaBERTa\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be911d",
   "metadata": {},
   "source": [
    "#### Tokenize and vectorize the text data using RoBERTa tokenizer and CountVectorizer for non-sequential features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = dataset['train']['text']\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords.words('english'), max_features=10000)\n",
    "vectorizer.fit(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978be80",
   "metadata": {},
   "source": [
    "#### Define HybridDataset Class and HybridModel- FeaRoberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vectorizer, tokenizer, max_token_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vectorizer = vectorizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_token_len,\n",
    "            return_token_type_ids=False, padding='max_length', return_attention_mask=True,\n",
    "            return_tensors='pt', truncation=True)\n",
    "\n",
    "        vectorized_text = torch.tensor(self.vectorizer.transform([text]).toarray(), dtype=torch.float32).squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'vectorized_text': vectorized_text,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRoBERTaModel(nn.Module):\n",
    "    def __init__(self, roberta_model_name, num_vectorized_features):\n",
    "        super(HybridRoBERTaModel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        roberta_output_size = self.roberta.config.hidden_size\n",
    "\n",
    "        self.vectorized_feature_processor = nn.Sequential(\n",
    "            nn.Linear(num_vectorized_features, 128), nn.ReLU(), nn.Dropout(0.2))\n",
    "\n",
    "        self.classifier = nn.Linear(roberta_output_size + 128, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, vectorized_features):\n",
    "        with autocast():\n",
    "            roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            roberta_pooled_output = roberta_output.pooler_output\n",
    "            processed_vectorized_features = self.vectorized_feature_processor(vectorized_features)\n",
    "            combined_features = torch.cat((roberta_pooled_output, processed_vectorized_features), dim=1)\n",
    "            combined_features = self.dropout(combined_features)\n",
    "            logits = self.classifier(combined_features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad02e27",
   "metadata": {},
   "source": [
    "##### Tokenization and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', max_length=512)\n",
    "model = HybridRoBERTaModel('roberta-base', num_vectorized_features=10000)\n",
    "train_dataset = HybridDataset(dataset['train']['text'], dataset['train']['label'], vectorizer, tokenizer)\n",
    "valid_dataset = HybridDataset(dataset['valid']['text'], dataset['valid']['label'], vectorizer, tokenizer)\n",
    "test_dataset = HybridDataset(dataset['test']['text'], dataset['test']['label'], vectorizer, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46eccc9",
   "metadata": {},
   "source": [
    "#### Model Configuration and Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47942da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)  \n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples, scaler, accumulation_steps=4):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    \n",
    "    accumulation_counter = 0\n",
    "\n",
    "    for step, d in enumerate(data_loader):  \n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        vectorized_text = d[\"vectorized_text\"].to(device)\n",
    "        labels = d[\"labels\"].to(device).float()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                vectorized_features=vectorized_text\n",
    "            )\n",
    "            outputs = outputs.squeeze(-1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Perform the optimization step and reset gradients based on accumulation\n",
    "        accumulation_counter += 1\n",
    "        if accumulation_counter % accumulation_steps == 0 or step + 1 == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            accumulation_counter = 0\n",
    "\n",
    "        # Accuracy calculation (use sigmoid and round for binary classification)\n",
    "        correct_predictions += torch.sum(torch.round(torch.sigmoid(outputs)) == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "\n",
    "\n",
    "def valid_epoch(model, data_loader, loss_fn, device, n_examples):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            vectorized_text = d[\"vectorized_text\"].to(device)\n",
    "            labels = d[\"labels\"].to(device).float()  # Convert labels to float\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                vectorized_features=vectorized_text\n",
    "            )\n",
    "            outputs = outputs.squeeze(-1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(torch.round(torch.sigmoid(outputs)) == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b783b2",
   "metadata": {},
   "source": [
    "#### Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "def valid_epoch(model, data_loader, loss_fn, device, n_examples):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    task_type = 'binary'\n",
    "    \n",
    "    # Initialize torchmetrics accumulators\n",
    "    precision_metric = torchmetrics.Precision(task_type, threshold=0.6).to(device)\n",
    "    recall_metric = torchmetrics.Recall(task_type, threshold=0.6).to(device)\n",
    "    f1_metric = torchmetrics.F1Score(task_type, threshold=0.6).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            vectorized_text = d[\"vectorized_text\"].to(device)\n",
    "            labels = d[\"labels\"].to(device).float()\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                vectorized_features=vectorized_text\n",
    "            )\n",
    "            outputs = outputs.squeeze(-1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Update torchmetrics accumulators\n",
    "            precision_metric(predictions, labels.int())\n",
    "            recall_metric(predictions, labels.int())\n",
    "            f1_metric(predictions, labels.int())\n",
    "\n",
    "    accuracy = correct_predictions.double() / n_examples\n",
    "    precision = precision_metric.compute()\n",
    "    recall = recall_metric.compute()\n",
    "    f1 = f1_metric.compute()\n",
    "\n",
    "    # Reset metrics for next epoch\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61441bf",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b89897",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "best_accuracy = 0\n",
    "accumulation_steps = 16\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model, train_loader, loss_fn, optimizer, device, len(train_dataset), scaler)\n",
    "\n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "\n",
    "    \n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "\n",
    "    \n",
    "    val_acc, val_loss, val_f1, val_recall, val_precision = valid_epoch(\n",
    "        model, valid_loader, loss_fn, device, len(valid_dataset))\n",
    "\n",
    "    print(f\"Val loss {val_loss} accuracy {val_acc} F1 {val_f1} Recall {val_recall} Precision {val_precision}\")\n",
    "    \n",
    "    \n",
    "   \n",
    "    if val_acc > best_accuracy:\n",
    "        save_checkpoint(model, optimizer, 'best_model_state.bin')\n",
    "        best_accuracy = val_acc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
